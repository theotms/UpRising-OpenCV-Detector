#include <opencv2/opencv.hpp>
#include <opencv2/aruco.hpp>
#include <fstream>
#include <iostream>
#include <nlohmann/json.hpp>
#include <thread>
#include <atomic>
#include <mutex>
#include <map>

using json = nlohmann::json;
using namespace std;
using namespace cv;

// ---- Arena Marker IDs ----
const int TOP_LEFT_ID = 46;
const int TOP_RIGHT_ID = 47;
const int BOTTOM_RIGHT_ID = 48;
const int BOTTOM_LEFT_ID = 49;

// ---- Shared variables for threaded frame capture ----
std::atomic<bool> running(true);  // Flag to stop capture thread
cv::Mat sharedFrame;              // Shared captured frame
std::mutex frameMutex;            // Frame access mutex

// ---- Capture thread function ----
void captureLoop(cv::VideoCapture& cap) {
    cv::Mat local;
    while (running) {
        cap >> local;
        if (local.empty()) continue;

        std::lock_guard<std::mutex> lock(frameMutex);
        local.copyTo(sharedFrame);
    }
}

int main() {
    // ---- Load camera calibration data from JSON file ----
    Mat cameraMatrix, distCoeffs;
    {
        ifstream file("rpi-camera-calib-params.json");
        if (!file) {
            cerr << "Calibration file not found!" << endl;
            return -1;
        }

        json calib;
        file >> calib;

        cameraMatrix = Mat(3, 3, CV_32F);
        distCoeffs = Mat(1, 5, CV_32F);

        auto mtx = calib["mtx"];
        auto dist = calib["dist"];

        for (int i = 0; i < 3; i++)
            for (int j = 0; j < 3; j++)
                cameraMatrix.at<float>(i, j) = mtx[i][j];

        for (int i = 0; i < 5; i++)
            distCoeffs.at<float>(0, i) = dist[i];
    }

    // ---- Set marker size in meters ----
    float markerLength = 0.105f;

    // ---- Configure ArUco dictionary and detection parameters ----
    Ptr<aruco::Dictionary> dictionary = aruco::getPredefinedDictionary(aruco::DICT_4X4_50);
    Ptr<aruco::DetectorParameters> parameters = makePtr<aruco::DetectorParameters>();

    parameters->cornerRefinementMethod = aruco::CORNER_REFINE_SUBPIX;
    parameters->cornerRefinementWinSize = 5;
    parameters->minMarkerDistanceRate = 0.05;
    parameters->cornerRefinementMinAccuracy = 0.5;

    // ---- Open camera device ----
    cv::VideoCapture cap("/dev/video0", cv::CAP_V4L2);
    if (!cap.isOpened()) {
        std::cerr << "Could not open /dev/video0" << std::endl;
        return -1;
    }

    cap.set(cv::CAP_PROP_FRAME_WIDTH, 1920);
    cap.set(cv::CAP_PROP_FRAME_HEIGHT, 1080);

    std::thread captureThread(captureLoop, std::ref(cap));

    // ---- Main detection loop ----
    while (true) {
        cv::Mat frame;
        {
            std::lock_guard<std::mutex> lock(frameMutex);
            if (!sharedFrame.empty())
                sharedFrame.copyTo(frame);
        }

        if (frame.empty()) continue;

        vector<int> ids;
        vector<vector<Point2f>> corners, rejected;
        aruco::detectMarkers(frame, dictionary, corners, ids, parameters, rejected);

        if (!ids.empty()) {
            vector<Vec3d> rvecs, tvecs;
            aruco::estimatePoseSingleMarkers(corners, markerLength, cameraMatrix, distCoeffs, rvecs, tvecs);

            aruco::drawDetectedMarkers(frame, corners, ids);
            std::map<int, Point2f> markerCenters;

            for (size_t i = 0; i < ids.size(); ++i) {
                Point2f center(0, 0);
                for (const auto& pt : corners[i]) {
                    center += pt;
                }
                center *= 0.25f;
                markerCenters[ids[i]] = center;

                circle(frame, center, 4, Scalar(255, 0, 0), -1);
                putText(frame, to_string(ids[i]), center + Point2f(5, -5), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(0, 255, 255), 1);
                drawFrameAxes(frame, cameraMatrix, distCoeffs, rvecs[i], tvecs[i], markerLength * 0.5f);
            }

            // ---- Draw green lines between corners ----
            if (markerCenters.count(TOP_LEFT_ID) && markerCenters.count(TOP_RIGHT_ID) &&
                markerCenters.count(BOTTOM_RIGHT_ID) && markerCenters.count(BOTTOM_LEFT_ID)) {

                line(frame, markerCenters[TOP_LEFT_ID], markerCenters[TOP_RIGHT_ID], Scalar(0, 255, 0), 2);
                line(frame, markerCenters[TOP_RIGHT_ID], markerCenters[BOTTOM_RIGHT_ID], Scalar(0, 255, 0), 2);
                line(frame, markerCenters[BOTTOM_RIGHT_ID], markerCenters[BOTTOM_LEFT_ID], Scalar(0, 255, 0), 2);
                line(frame, markerCenters[BOTTOM_LEFT_ID], markerCenters[TOP_LEFT_ID], Scalar(0, 255, 0), 2);

                // ---- Homography: Warp to top-down view ----
                vector<Point2f> srcPts = {
                    markerCenters[TOP_LEFT_ID],
                    markerCenters[TOP_RIGHT_ID],
                    markerCenters[BOTTOM_RIGHT_ID],
                    markerCenters[BOTTOM_LEFT_ID]
                };

                float arenaSizePx = 600.0f;
                vector<Point2f> dstPts = {
                    Point2f(0, 0),
                    Point2f(arenaSizePx, 0),
                    Point2f(arenaSizePx, arenaSizePx),
                    Point2f(0, arenaSizePx)
                };

                Mat H = getPerspectiveTransform(srcPts, dstPts);
                Mat topDownView;
                warpPerspective(frame, topDownView, H, Size(arenaSizePx, arenaSizePx));
                imshow("Top-Down Arena View", topDownView);
            }
        }

        imshow("Aruco Detection", frame);
        if (waitKey(1) == 'q') break;
    }

    running = false;
    captureThread.join();
    return 0;
}
